{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Feature Importance and Tree Interpretation \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook for the lecture 4\n",
    "\n",
    "**Revisiting the hyper parameter tuning.**\n",
    " Here we are reading about summarizing the relation between hyperparameter, colinearity etc., that help the models\n",
    "\n",
    "1. set_rf_samples(n): Each individual estimator is as accurate as possible. Across the estimator the correlation is as low as possible. Execution would be quick\n",
    "    \n",
    "2. min_samples_leaf: This basically defines the number of values that should be present in the leaf node. Higher the value lower the depth. The depth or the value doesn't necessarily have to correspond to the accuracy. Sometimes due to lower value it could lead to overfit. Usual min leaf values are - 1,3,5,10,25,100\n",
    "\n",
    "3. max_features: This allows the model to choose from a perentage of features at every split. So if max_features=0.5 means that at every split it selects from a fresh set of 50% features. note: if max features is sqrt or log2 then it helps in reducing the error. So what we can use here are:\n",
    "    * sqrt\n",
    "    * log\n",
    "    * 0.5\n",
    "\n",
    "4. n_jobs: it is the number of cores you want to use to run the algorithm\n",
    "\n",
    "5. oob_score: out of bag sampling is done to use sampling of data to validate the data\n",
    "\n",
    "These are pretty much the important hyper parameters to tweak to have good model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "* Feature importance can be extracted using the libraries from sklearn. This helps in identifying the which feature adds more value to the model. The corelations between features can be found and there by help the model to have better features that would inturn help the model to have better performance. \n",
    "\n",
    "***Note:** Why can't we go feature by feature and there by select all the features that would contribute to a better performing model?*\n",
    "This can't be advised as it will get rid of the interactions that can be present between the features and this might result in loosing a potential dataadd to the model.\n",
    "\n",
    "* Feature importance can be obtained by plotting the bar graphs, so now with those insights try playing around by removing certain set of features and check how the feature importance changed. With the help of these insights over the feature we can then work with using feature engineering.\n",
    "\n",
    "***Note:** Subject matter expertise is very important that helps get intimate relations between features that will help in building better models.* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is the feature importance done in real time project?**\n",
    "\n",
    "Usual approach is assuming a relation between the dependent and independent variables. This assumption of considering a linear/ logistic relation between dependent and independent variables results in potentially creating coefficients of the features could be very biased with models. So using classical statisctal techniques to observe the feature importance without preprocessing and not understanding the interaction between the variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "One hot encoding is typically a way to convert the non integer variables into integer variable that could be given to the model to work on. This method of one hot encoding helps in creating a split functionality for the tree easily as the split with just a simple encoding will result in a more depth i.e., using one hot encoding we can split easily with the value otherwise the model has to split multiple times till we arrive at the necessary split.\n",
    "\n",
    "**max_n_cat** parameter in the proc_df is used to have the one hot encoding which could be used by the model to use it. Here the value coressponding to the parameter is one hot encoding all the categorical values whose cardinality is less or equal to the threshold mentioned\n",
    ">if max_n_cat = 7; we one hot encode all the categorical columns having the cardinality less than equal to 7\n",
    "\n",
    "Sometimes one hot encoding helps in creating a better performing model and sometimes it can contribute in understanding the feature importance better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix\n",
    "Feature importance could be selected using the correlation between 2 features to understand how similar the features are to each other and using that data we can either do feature engineering or create a dataset. To understand this we have to use ranked correlation to check the relation between the features; the most used is the **spearmanr** to find the ranked correlation. So to see the relations we create a **dendogram**\n",
    "\n",
    "So using the dendogram we remove the features that are closely related to each other and calcuate the oob score and check the model performance by removing the features one by one. This way we can check with more combinations and check the model for the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Partial dependence\n",
    " \n",
    " This is a technique which helps us understand the relation between the important features and the dependent variable. ***Remember** with the data you have perform as many univariate analysis as possible to understand what is happening with the data you have* \n",
    " \n",
    " Partial dependence plot which is a plotting technique to understand the evloution of the different data i.e., understand how a data would do in a specific situation, this way we will be able to understand the different business insights/ create interactions/ feature selection. \n",
    " \n",
    "The PDP chart helps more in real time business insights that expresses what is the driving force for any industry and also understand different real time variable which can be missed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "                            x End of Notes x\n",
    "---\n",
    "\n",
    "*Note: Tree interpreter concept is carried forward in the next lecture*\n",
    "\n",
    "\n",
    "\n",
    "*This notebook is a self notes written by Surya Viswanath Perala, it is last updated on 26th April, 2020. This notebook will further be updated to understand more concepts and also the implementation of different concepts explained.*\n",
    "\n",
    "Thank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
